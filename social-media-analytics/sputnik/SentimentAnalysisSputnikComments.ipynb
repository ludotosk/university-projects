{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Social Media Analytics\n",
    "### Introduction to Text Mining\n",
    "## Sentiment Analysis\n",
    "(c) Nuno Antonio 2019-2022 v1.02"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import csv\n",
    "import pickle\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from deep_translator import GoogleTranslator\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from tqdm import tqdm\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = pd.read_parquet(\"sputnikComments.parquet.snappy\", engine=\"fastparquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>link</th>\n",
       "      <th>comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/20230501/kiev-lost-over-300-soldiers-over-pas...</td>\n",
       "      <td>330 US mercenaries with mostly Ukrainian passp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/20230501/kiev-lost-over-300-soldiers-over-pas...</td>\n",
       "      <td>330?? figures are coming down, supposed to be ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/20230430/russia-destroys-up-to-200-tonnes-of-...</td>\n",
       "      <td>Very soon they will only have stones to throw ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/20230430/russia-destroys-up-to-200-tonnes-of-...</td>\n",
       "      <td>why destroy it when you can put it to good use...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/20230430/russian-forces-discover-underground-...</td>\n",
       "      <td>Kudos to the Russian explosive ordnance dispos...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                link  \\\n",
       "0  /20230501/kiev-lost-over-300-soldiers-over-pas...   \n",
       "1  /20230501/kiev-lost-over-300-soldiers-over-pas...   \n",
       "2  /20230430/russia-destroys-up-to-200-tonnes-of-...   \n",
       "3  /20230430/russia-destroys-up-to-200-tonnes-of-...   \n",
       "4  /20230430/russian-forces-discover-underground-...   \n",
       "\n",
       "                                             comment  \n",
       "0  330 US mercenaries with mostly Ukrainian passp...  \n",
       "1  330?? figures are coming down, supposed to be ...  \n",
       "2  Very soon they will only have stones to throw ...  \n",
       "3  why destroy it when you can put it to good use...  \n",
       "4  Kudos to the Russian explosive ordnance dispos...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1045 entries, 0 to 1044\n",
      "Data columns (total 2 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   link     1045 non-null   object\n",
      " 1   comment  1045 non-null   object\n",
      "dtypes: object(2)\n",
      "memory usage: 16.5+ KB\n"
     ]
    }
   ],
   "source": [
    "ds.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text preprocessing\n",
    "def textPreProcess(\n",
    "    rawText,\n",
    "    removeHTML=True,\n",
    "    charsToRemove=r\"\\?|\\.|\\!|\\;|\\.|\\\"|\\,|\\(|\\)|\\&|\\:|\\-\",\n",
    "    removeNumbers=True,\n",
    "    removeLineBreaks=False,\n",
    "    specialCharsToRemove=r\"[^\\x00-\\xfd]\",\n",
    "    convertToLower=True,\n",
    "    removeConsecutiveSpaces=True,\n",
    "):\n",
    "    if type(rawText) != str:\n",
    "        return rawText\n",
    "    procText = rawText\n",
    "\n",
    "    # Remove HTML\n",
    "    if removeHTML:\n",
    "        procText = BeautifulSoup(procText, \"html.parser\").get_text()\n",
    "\n",
    "    # Remove punctuation and other special characters\n",
    "    if len(charsToRemove) > 0:\n",
    "        procText = re.sub(charsToRemove, \" \", procText)\n",
    "\n",
    "    # Remove numbers\n",
    "    if removeNumbers:\n",
    "        procText = re.sub(r\"\\d+\", \" \", procText)\n",
    "\n",
    "    # Remove line breaks\n",
    "    if removeLineBreaks:\n",
    "        procText = procText.replace(\"\\n\", \" \").replace(\"\\r\", \"\")\n",
    "\n",
    "    # Remove special characters\n",
    "    if len(specialCharsToRemove) > 0:\n",
    "        procText = re.sub(specialCharsToRemove, \" \", procText)\n",
    "\n",
    "    # Normalize to lower case\n",
    "    if convertToLower:\n",
    "        procText = procText.lower()\n",
    "\n",
    "    # Replace multiple consecutive spaces with just one space\n",
    "    if removeConsecutiveSpaces:\n",
    "        procText = re.sub(\" +\", \" \", procText)\n",
    "\n",
    "    return procText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize words\n",
    "def tokenize_words(words):\n",
    "    if (type(words) != str) or (word_tokenize(words) == \"\"):\n",
    "        return np.nan\n",
    "    else:\n",
    "        return word_tokenize(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create text from words\n",
    "def recreateText(words):\n",
    "    if type(words) == list:\n",
    "        temp_str = (\" \").join(words)\n",
    "        return temp_str\n",
    "    else:\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to break texts into sentences\n",
    "def tokenize_sentences(texts):\n",
    "    s_token = sent_tokenize(texts)\n",
    "    return s_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to remove stop words\n",
    "def removeStopWords(t, stop_words):\n",
    "    if type(t) == list:\n",
    "        return [w for w in t if not w in stop_words]\n",
    "    else:\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Because a review can express multiple opinions, let's analyze opinions by sentence\n",
    "\n",
    "# Break reviews' into a list of lists sentencesc\n",
    "listOfSentences = ds.comment.apply(tokenize_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe with only the description\n",
    "processedReviews = pd.DataFrame(\n",
    "    data=ds.comment.apply(\n",
    "        textPreProcess, charsToRemove=\"\", removeLineBreaks=False, removeNumbers=False\n",
    "    ).values,\n",
    "    index=ds.index,\n",
    "    columns=[\"PreProcessedText\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Very soon they will only have stones to throw at the advancing Russians.'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check first review\n",
    "ds.comment[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Very soon they will only have stones to throw at the advancing Russians.']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sentences of first review\n",
    "listOfSentences[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame for sentences\n",
    "sentences = pd.DataFrame(\n",
    "    data=[item for elem in listOfSentences for item in elem], columns=[\"BaseText\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a column with the review ID\n",
    "sentencesPerReview = []\n",
    "for elem in listOfSentences:\n",
    "    sentencesPerReview.append(len(elem))\n",
    "sentences[\"link\"] = np.repeat(ds[\"link\"].values, sentencesPerReview)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess text\n",
    "sentences[\"PreProcessedText\"] = sentences[\"BaseText\"].apply(textPreProcess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get words\n",
    "sentences[\"Words\"] = sentences[\"PreProcessedText\"].apply(tokenize_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove stopwords\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "sentences[\"WordsCleaned\"] = sentences[\"Words\"].apply(\n",
    "    removeStopWords, stop_words=stop_words\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recreate sentence without stopwords\n",
    "sentences[\"ProcessedText\"] = sentences[\"WordsCleaned\"].apply(recreateText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sentiment analysis object\n",
    "analyser = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "us mercenaries mostly ukrainian passports {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}\n"
     ]
    }
   ],
   "source": [
    "# To test, let's evaluate first sentence of first review\n",
    "# Scales:\n",
    "#   compound: -1:most extreme negative, 1:most extreme positive\n",
    "#     positive: compound >=0.05\n",
    "#     neutral: -0.05<compound<0.05\n",
    "#     negative: compound <= -0.05\n",
    "#   pos, neu, neg: proportion of text that are positive, neutral or negative\n",
    "score = analyser.polarity_scores(sentences[\"ProcessedText\"][0])\n",
    "print(sentences[\"ProcessedText\"][0], score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nopen_file = open(\"sentences-sputnik.pkl\", \"rb\")\\nsentences = pickle.load(open_file)\\nopen_file.close()\\n'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Computing sentences sentiment requires around 40 minutes, using this file we have the saved results.\n",
    "\"\"\"\n",
    "open_file = open(\"comments-sputnik.pkl\", \"rb\")\n",
    "sentences = pickle.load(open_file)\n",
    "open_file.close()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2273/2273 [04:44<00:00,  7.98it/s]\n"
     ]
    }
   ],
   "source": [
    "# Process sentiment for all sentences\n",
    "all_scores = []\n",
    "for t in tqdm(sentences[\"ProcessedText\"]):\n",
    "    try:\n",
    "        score = analyser.polarity_scores(t)\n",
    "        all_scores.append(score)\n",
    "    except:\n",
    "        all_scores.append(dict({\"neg\": 0.0, \"neu\": 0.0, \"pos\": 0.0, \"compound\": 0.0}))\n",
    "sentences[\"Sentiment\"] = [c[\"compound\"] for c in all_scores]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"comments-sputnik.pkl\"\n",
    "\n",
    "open_file = open(file_name, \"wb\")\n",
    "pickle.dump(sentences, open_file)\n",
    "open_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_547933/2201009863.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  ds[\"Sentiment\"] = meanByReview[ds[\"link\"]].values\n"
     ]
    }
   ],
   "source": [
    "# Compute review's sentiment as the mean sentiment from its sentences\n",
    "meanByReview = sentences.groupby(\"link\")[\"Sentiment\"].mean()\n",
    "\n",
    "# Consider sentences with no result as neutral (0)\n",
    "meanByReview = meanByReview.fillna(0)\n",
    "\n",
    "# Add column Sentiment to reviews Dataframe\n",
    "ds[\"Sentiment\"] = meanByReview[ds[\"link\"]].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_547933/3879556559.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  ds[\"Polarity\"] = x\n"
     ]
    }
   ],
   "source": [
    "# Assign a qualitative evaluation to the review\n",
    "bins = pd.IntervalIndex.from_tuples(\n",
    "    [(-1.1, -0.05), (-0.05, 0.05), (0.05, 1)], closed=\"right\"\n",
    ")\n",
    "x = pd.cut(ds[\"Sentiment\"].to_list(), bins)\n",
    "x.categories = [\"Negative\", \"Neutral\", \"Positive\"]\n",
    "ds[\"Polarity\"] = x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>link</th>\n",
       "      <th>comment</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Polarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/20230501/kiev-lost-over-300-soldiers-over-pas...</td>\n",
       "      <td>330 US mercenaries with mostly Ukrainian passp...</td>\n",
       "      <td>-0.14985</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/20230501/kiev-lost-over-300-soldiers-over-pas...</td>\n",
       "      <td>330?? figures are coming down, supposed to be ...</td>\n",
       "      <td>-0.14985</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/20230430/russia-destroys-up-to-200-tonnes-of-...</td>\n",
       "      <td>Very soon they will only have stones to throw ...</td>\n",
       "      <td>-0.31845</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/20230430/russia-destroys-up-to-200-tonnes-of-...</td>\n",
       "      <td>why destroy it when you can put it to good use...</td>\n",
       "      <td>-0.31845</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/20230430/russian-forces-discover-underground-...</td>\n",
       "      <td>Kudos to the Russian explosive ordnance dispos...</td>\n",
       "      <td>0.13190</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                link  \\\n",
       "0  /20230501/kiev-lost-over-300-soldiers-over-pas...   \n",
       "1  /20230501/kiev-lost-over-300-soldiers-over-pas...   \n",
       "2  /20230430/russia-destroys-up-to-200-tonnes-of-...   \n",
       "3  /20230430/russia-destroys-up-to-200-tonnes-of-...   \n",
       "4  /20230430/russian-forces-discover-underground-...   \n",
       "\n",
       "                                             comment  Sentiment  Polarity  \n",
       "0  330 US mercenaries with mostly Ukrainian passp...   -0.14985  Negative  \n",
       "1  330?? figures are coming down, supposed to be ...   -0.14985  Negative  \n",
       "2  Very soon they will only have stones to throw ...   -0.31845  Negative  \n",
       "3  why destroy it when you can put it to good use...   -0.31845  Negative  \n",
       "4  Kudos to the Russian explosive ordnance dispos...    0.13190  Positive  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.to_parquet(\"sputnikSentimentComment.parquet.snappy\", engine=\"fastparquet\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "d7a18bc08bf6b314cad8b0dd8f53415ad78d1015cc806d14e4873c235fb4e191"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
